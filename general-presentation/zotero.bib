
@inproceedings{gori_new_2005,
	title = {A new model for learning in graph domains},
	volume = {2},
	doi = {10.1109/IJCNN.2005.1555942},
	abstract = {In several applications the information is naturally represented by graphs. Traditional approaches cope with graphical data structures using a preprocessing phase which transforms the graphs into a set of flat vectors. However, in this way, important topological information may be lost and the achieved results may heavily depend on the preprocessing stage. This paper presents a new neural model, called graph neural network ({GNN}), capable of directly processing graphs. {GNNs} extends recursive neural networks and can be applied on most of the practically useful kinds of graphs, including directed, undirected, labelled and cyclic graphs. A learning algorithm for {GNNs} is proposed and some experiments are discussed which assess the properties of the model.},
	eventtitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	pages = {729--734 vol. 2},
	booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
	author = {Gori, M. and Monfardini, G. and Scarselli, F.},
	date = {2005-07},
	note = {{ISSN}: 2161-4407},
	keywords = {Application software, data structures, Data structures, Encoding, Focusing, graph neural network, graph theory, graphical data structures, learning (artificial intelligence), learning algorithm, Machine learning, Machine learning algorithms, neural nets, Neural networks, Recurrent neural networks, recursive neural networks, Software engineering, Tree graphs},
	file = {IEEE Xplore Abstract Record:/home/marekdedic/Zotero/storage/33929VKF/1555942.html:text/html},
}

@unpublished{velickovic_opening_2020,
	location = {Vienna},
	title = {Opening Remarks},
	url = {https://slideslive.at/icml-2020/graph-representation-learning-and-beyond-grl},
	note = {{ICML} 2020},
	author = {Veličković, Petar and Deac, Andreea},
	date = {2020-07-17},
}

@online{kubara_machine_2020,
	title = {Machine Learning Tasks on Graphs},
	url = {https://towardsdatascience.com/machine-learning-tasks-on-graphs-7bc8f175119a},
	abstract = {Can We Divide It Into Supervised/Unsupervised Learning? It’s Not That Simple…},
	titleaddon = {Medium},
	author = {Kubara, Kacper},
	urldate = {2020-11-03},
	date = {2020-09-28},
	langid = {english},
	file = {Snapshot:/home/marekdedic/Zotero/storage/4IYMYT34/machine-learning-tasks-on-graphs-7bc8f175119a.html:text/html},
}

@article{girvan_community_2002,
	title = {Community structure in social and biological networks},
	volume = {99},
	rights = {Copyright © 2002, The National Academy of Sciences},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/99/12/7821},
	doi = {10.1073/pnas.122653799},
	abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known—a collaboration network and a food web—and find that it detects significant and informative community divisions in both cases.},
	pages = {7821--7826},
	number = {12},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Girvan, M. and Newman, M. E. J.},
	urldate = {2020-11-03},
	date = {2002-06-11},
	langid = {english},
	pmid = {12060727},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	file = {Full Text PDF:/home/marekdedic/Zotero/storage/HFJ9E5AE/Girvan and Newman - 2002 - Community structure in social and biological netwo.pdf:application/pdf;Snapshot:/home/marekdedic/Zotero/storage/T8JFLUPY/7821.html:text/html},
}

@article{zhou_graph_2019,
	title = {Graph Neural Networks: A Review of Methods and Applications},
	url = {http://arxiv.org/abs/1812.08434},
	shorttitle = {Graph Neural Networks},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks ({GNNs}) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive {GNNs} have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network ({GCN}), graph attention network ({GAT}), gated graph neural network ({GGNN}) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
	journaltitle = {{arXiv}:1812.08434 [cs, stat]},
	author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	urldate = {2020-11-03},
	date = {2019-07-10},
	eprinttype = {arxiv},
	eprint = {1812.08434},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/marekdedic/Zotero/storage/7J39JVPN/Zhou et al. - 2019 - Graph Neural Networks A Review of Methods and App.pdf:application/pdf;arXiv.org Snapshot:/home/marekdedic/Zotero/storage/KLTUI2U8/1812.html:text/html},
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {https://openreview.net/forum?id=idpCdOWtqXd60},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity...},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2020-11-03},
	date = {2013-01-17},
	langid = {english},
	file = {Snapshot:/home/marekdedic/Zotero/storage/KERQU498/forum.html:text/html},
}

@inproceedings{perozzi_deepwalk_2014,
	title = {Deepwalk: Online learning of social representations},
	shorttitle = {Deepwalk},
	pages = {701--710},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
	date = {2014},
	file = {Full Text:/home/marekdedic/Zotero/storage/ISN68YG2/Perozzi et al. - 2014 - Deepwalk Online learning of social representation.pdf:application/pdf;Snapshot:/home/marekdedic/Zotero/storage/Y3TWLT7L/2623330.html:text/html},
}